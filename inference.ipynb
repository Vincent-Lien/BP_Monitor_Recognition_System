{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1416ddf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "def localization_and_crop_image(\n",
    "    img_path, \n",
    "    model_path='checkpoints/localization_best.pt',\n",
    "    class_names=None,\n",
    "    save_classes=None,\n",
    "    conf_start=0.50,\n",
    "    conf_min=0.05,\n",
    "    imgsz=640,\n",
    "    device='0',\n",
    "    verbose=False\n",
    "):\n",
    "    # Set default values if not provided\n",
    "    if class_names is None:\n",
    "        class_names = ['DIA', 'PUL', 'SYS', 'Sphygmomanometer']\n",
    "    \n",
    "    if save_classes is None:\n",
    "        save_classes = [0, 1, 2]  # 0: DIA, 1: PUL, 2: SYS\n",
    "    \n",
    "    # Load model\n",
    "    model = YOLO(model_path)\n",
    "    \n",
    "    # Read image\n",
    "    img = cv2.imread(img_path)\n",
    "    \n",
    "    conf = conf_start\n",
    "    successful = False\n",
    "    cropped_images = {}  # Dictionary to store cropped images\n",
    "    \n",
    "    while conf >= conf_min and not successful:\n",
    "        if verbose:\n",
    "            print(f\"Trying with confidence threshold: {conf:.2f}\")\n",
    "        \n",
    "        # Clear previous results\n",
    "        cropped_images = {}\n",
    "        \n",
    "        # Perform inference\n",
    "        results = model(img_path, imgsz=imgsz, conf=conf, device=device, verbose=verbose)\n",
    "        \n",
    "        # Process results\n",
    "        for i, result in enumerate(results):\n",
    "            boxes = result.boxes.xyxy.cpu().numpy()\n",
    "            classes = result.boxes.cls.cpu().numpy().astype(int)\n",
    "            \n",
    "            for j, (box, cls) in enumerate(zip(boxes, classes)):\n",
    "                if cls in save_classes:\n",
    "                    x1, y1, x2, y2 = map(int, box)\n",
    "                    crop_img = img[y1:y2, x1:x2]\n",
    "                    \n",
    "                    class_name = class_names[cls]\n",
    "                    if class_name not in cropped_images:\n",
    "                        cropped_images[class_name] = []\n",
    "                    \n",
    "                    cropped_images[class_name].append(crop_img)\n",
    "        \n",
    "        # Check if each class has exactly one image\n",
    "        all_classes_have_one_image = True\n",
    "        for cls in [class_names[i] for i in save_classes]:\n",
    "            if cls not in cropped_images or len(cropped_images[cls]) != 1:\n",
    "                images_count = 0 if cls not in cropped_images else len(cropped_images[cls])\n",
    "                all_classes_have_one_image = False\n",
    "                if verbose:\n",
    "                    print(f\"Class {cls} has {images_count} images, expecting 1\")\n",
    "                break\n",
    "        \n",
    "        if all_classes_have_one_image:\n",
    "            successful = True\n",
    "            if verbose:\n",
    "                print(f\"Successfully found one image for each class with conf={conf:.2f}\")\n",
    "        else:\n",
    "            conf -= 0.01\n",
    "    \n",
    "    if not successful:\n",
    "        if verbose:\n",
    "            print(f\"Could not find exactly one image for each class. Minimum conf {conf_min} reached.\")\n",
    "        return None\n",
    "    \n",
    "    # Extract the single image for each class\n",
    "    # Extract the single image for each class in the specified order\n",
    "    result_images = {}\n",
    "    for cls in ['SYS', 'DIA', 'PUL']:\n",
    "        if cls in cropped_images and len(cropped_images[cls]) > 0:\n",
    "            result_images[cls] = cropped_images[cls][0]\n",
    "    \n",
    "    return result_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8b1af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def cut_out_3_numbers(image, show=False):\n",
    "    reshaped_image = cv2.resize(image, (1000, 500))\n",
    "    # Extract three segments of the image based on x-coordinates\n",
    "    first_segment = reshaped_image[:, 0:210, :]\n",
    "    second_segment = reshaped_image[:, 210:600, :]\n",
    "    third_segment = reshaped_image[:, 600:1000, :]\n",
    "\n",
    "    if show:    \n",
    "        # Display the segments\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(cv2.cvtColor(first_segment, cv2.COLOR_BGR2RGB))\n",
    "        plt.title('First Segment (0-200)')\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(cv2.cvtColor(second_segment, cv2.COLOR_BGR2RGB))\n",
    "        plt.title('Second Segment (200-600)')\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(cv2.cvtColor(third_segment, cv2.COLOR_BGR2RGB))\n",
    "        plt.title('Third Segment (600-1000)')\n",
    "        plt.show()\n",
    "\n",
    "    return first_segment, second_segment, third_segment\n",
    "\n",
    "def cut_out_2_numbers(image, show=False):\n",
    "    reshaped_image = cv2.resize(image, (1000, 500))\n",
    "    # Extract three segments of the image based on x-coordinates\n",
    "    first_segment = reshaped_image[:, 0:500, :]\n",
    "    second_segment = reshaped_image[:, 500:1000, :]\n",
    "\n",
    "    if show:\n",
    "        # Display the segments\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(cv2.cvtColor(first_segment, cv2.COLOR_BGR2RGB))\n",
    "        plt.title('First Segment (0-500)')\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(cv2.cvtColor(second_segment, cv2.COLOR_BGR2RGB))\n",
    "        plt.title('Second Segment (500-1000)')\n",
    "        plt.show()\n",
    "\n",
    "    return first_segment, second_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04e0359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def load_model(model_path, device):\n",
    "    \"\"\"\n",
    "    Load the trained seven segment classifier model\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path to the saved model weights\n",
    "        device (torch.device): Device to run the model on\n",
    "        \n",
    "    Returns:\n",
    "        model (torch.nn.Module): Loaded model\n",
    "    \"\"\"\n",
    "    model = resnet18(weights=None)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, 10)\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def preprocess_image(image):\n",
    "    \"\"\"\n",
    "    Preprocess an image for inference\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file\n",
    "        \n",
    "    Returns:\n",
    "        tensor (torch.Tensor): Preprocessed image tensor\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    image = Image.fromarray(image).convert('RGB')\n",
    "    image_tensor = transform(image)\n",
    "    return image_tensor.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "def inference_single_image(model, image_path, device):\n",
    "    \"\"\"\n",
    "    Run inference on a single image\n",
    "    \n",
    "    Args:\n",
    "        model (torch.nn.Module): Loaded model\n",
    "        image_path (str): Path to the image file\n",
    "        device (torch.device): Device to run inference on\n",
    "        \n",
    "    Returns:\n",
    "        pred (int): Predicted digit\n",
    "        confidence (float): Confidence score\n",
    "    \"\"\"\n",
    "    image_tensor = preprocess_image(image_path).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor)\n",
    "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        confidence, prediction = torch.max(probabilities, 1)\n",
    "    \n",
    "    return prediction.item(), confidence.item()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model\n",
    "model = load_model('checkpoints/seven_seg_classification_best.pth', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91462003",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageEnhance\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def adjust_image(image, output_path=None, brightness_factor=1.0, contrast_factor=1.0, sharpness_factor=1.0, color_factor=1.0, show=False, save=False):\n",
    "    \"\"\"\n",
    "    調整圖片的亮度、對比度、銳利度和色彩飽和度。\n",
    "\n",
    "    Args:\n",
    "        image_path (str): 輸入圖片的路徑。\n",
    "        output_path (str): 輸出圖片的路徑。\n",
    "        brightness_factor (float): 亮度調整因子。1.0 表示不變，大於 1.0 變亮，小於 1.0 變暗。\n",
    "        contrast_factor (float): 對比度調整因子。1.0 表示不變，大於 1.0 增加對比，小於 1.0 降低對比。\n",
    "        sharpness_factor (float): 銳利度調整因子 (增豔)。1.0 表示不變，大於 1.0 增加銳利度，小於 1.0 模糊。\n",
    "        color_factor (float): 色彩飽和度調整因子 (增豔)。1.0 表示不變，大於 1.0 增加飽和度，小於 1.0 降低飽和度。\n",
    "    \"\"\"\n",
    "    # 打開圖片\n",
    "    image = Image.fromarray(image).convert(\"RGB\")  # 確保圖片是 RGB 模式\n",
    "\n",
    "    # 調整亮度 (亮部)\n",
    "    enhancer_b = ImageEnhance.Brightness(image)\n",
    "    img_bright = enhancer_b.enhance(brightness_factor)\n",
    "\n",
    "    # 調整對比度\n",
    "    enhancer_c = ImageEnhance.Contrast(img_bright)\n",
    "    img_contrast = enhancer_c.enhance(contrast_factor)\n",
    "\n",
    "    # 調整銳利度 (增豔 - 銳利度方面)\n",
    "    enhancer_s = ImageEnhance.Sharpness(img_contrast)\n",
    "    img_sharp = enhancer_s.enhance(sharpness_factor)\n",
    "\n",
    "    # 調整色彩飽和度 (增豔 - 色彩方面)\n",
    "    enhancer_col = ImageEnhance.Color(img_sharp)\n",
    "    img_final = enhancer_col.enhance(color_factor)\n",
    "    \n",
    "    if show:\n",
    "        # 展示原圖和調整後的圖片對比\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(np.array(image))\n",
    "        plt.title('Original Image')\n",
    "        plt.axis('off')\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(np.array(img_final))\n",
    "        plt.title('Adjusted Image')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    if output_path and save:\n",
    "        img_final.save(output_path)\n",
    "    \n",
    "    return np.array(img_final)  # 返回調整後的圖片數組"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5c94375",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "image_folder = 'my_images'\n",
    "\n",
    "results = []\n",
    "for image in sorted(os.listdir(image_folder)):\n",
    "\n",
    "    image = os.path.join(image_folder, image)\n",
    "    result_images = localization_and_crop_image(image)\n",
    "    \n",
    "    sys_image = result_images.get('SYS')\n",
    "    dia_image = result_images.get('DIA')\n",
    "    pul_image = result_images.get('PUL')\n",
    "\n",
    "    # Calculate 5% of the image height\n",
    "    crop_height = int(sys_image.shape[0] * 0.07)\n",
    "    sys_image = sys_image[crop_height:, :, :]\n",
    "\n",
    "    dia_image = adjust_image(dia_image, brightness_factor=1.5, contrast_factor=2.0, sharpness_factor=2.0, color_factor=1.0)\n",
    "    pul_image = adjust_image(pul_image, brightness_factor=2.0, contrast_factor=2.0, sharpness_factor=2.0, color_factor=1.0)\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(cv2.cvtColor(sys_image, cv2.COLOR_BGR2RGB))\n",
    "    plt.title('SYS')\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(cv2.cvtColor(dia_image, cv2.COLOR_BGR2RGB))\n",
    "    plt.title('DIA')\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(cv2.cvtColor(pul_image, cv2.COLOR_BGR2RGB))\n",
    "    plt.title('PUL')\n",
    "    plt.show()\n",
    "    \n",
    "    sys_number, dia_number, pul_number = '', '', ''\n",
    "\n",
    "    if sys_image.shape[1] / dia_image.shape[1] > 1.05:\n",
    "        sys_digits = cut_out_3_numbers(sys_image)\n",
    "    else:\n",
    "        sys_digits = cut_out_2_numbers(sys_image)\n",
    "\n",
    "    for digit in sys_digits:\n",
    "        prediction, _ = inference_single_image(model, digit, device)\n",
    "        sys_number += str(prediction)\n",
    "    sys_number = int(sys_number)\n",
    "    \n",
    "    dia_digits = cut_out_2_numbers(dia_image)\n",
    "    for digit in dia_digits:\n",
    "        prediction, _ = inference_single_image(model, digit, device)\n",
    "        dia_number += str(prediction)\n",
    "    dia_number = int(dia_number)\n",
    "\n",
    "    pul_digits = cut_out_2_numbers(pul_image)\n",
    "    for digit in pul_digits:\n",
    "        prediction, _ = inference_single_image(model, digit, device)\n",
    "        pul_number += str(prediction)\n",
    "    pul_number = int(pul_number)\n",
    "    \n",
    "    print(f\"image: {image}, SYS: {sys_number}, DIA: {dia_number}, PUL: {pul_number}\")\n",
    "    results.append((sys_number, dia_number, pul_number))\n",
    "\n",
    "    # break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bp_monitor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
